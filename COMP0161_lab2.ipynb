{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comp0161/colab/blob/main/COMP0161_lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwpPRf_mNsZ4"
      },
      "source": [
        "# Generating Music with Deep Learning (Part 2)\n",
        "\n",
        "In this, the second of three lab sessions, we will use the dataset created last week to train a **deep neural network** to generate music.\n",
        "\n",
        "Once again we will be making use of [Google's Colab computing environment](https://colab.research.google.com/#). Unlike the previous lab, the code this week is very computationally intensive, so you will want to ensure that the notebook connects to a **GPU-enabled** virtual machine. This should already be configured, but it's worth checking if you find the model training is taking a really long time.\n",
        "\n",
        "Once again, the code for this lab is in [Python](https://docs.python.org/3/tutorial/index.html). You do not need to know Python to complete the lab, but there will be some optional extra tasks you can try if you are comfortable with Python coding and want to explore further."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QVbwyRWKgox"
      },
      "source": [
        "# Background\n",
        "\n",
        "**Machine Learning** (ML) is an umbrella term for a range of techniques that distil structural and behavioural *patterns* from data â€” patterns that can then be used to understand and make predictions about future data drawn from the same population. The learned patterns are often conceptualised *probabilistically* as the **distribution** of the training data: ie, how likely any given sample of data is within the population as a whole.\n",
        "\n",
        "**Deep Learning** is a particular subset of ML, making use of large, flexible models known as **neural networks**, which are capable of representing a very broad range of complex behaviours. Neural networks are trained *incrementally*, taking multiple passes through the training data, each time making small changes to the model parameters to make the model outputs a little *less wrong*. This process is called **gradient descent**, where the gradient in question is the slope of wrongness.\n",
        "\n",
        "Deep Learning can be applied to a huge of array of different kinds of problems, and has produced many remarkable successes in recent years, but it is computationally demanding and very heavily dependent on the available training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOXredrFKxbF"
      },
      "source": [
        "## Language modelling\n",
        "\n",
        "One area to which Deep Learning has been very successfully applied is that of [Language Modelling](https://en.wikipedia.org/wiki/Language_model): learning the *joint probability distribution* of words in a language such as English. Roughly, the problem boils down to: given some amount of sentence context â€” say, the previous 10 or 100 words â€” what is the *next* word likely to be?\n",
        "\n",
        "The *meaning* of a sentence is determined by the choice and arrangement of its words, so knowing the patterns of occurrence reveals *something* about the syntax and semantics of the language. Exactly what and how much information is gained this way remains a matter of heated philosophical debate, but very large language models like [GPT-3](https://en.wikipedia.org/wiki/GPT-3) are certainly able to capture and at least superficially mimic a great deal of the form, content and general *feel* of natural language text.\n",
        "\n",
        "The language modelling approach is not restricted only to text, but can be applied to any kind of data that can be considered as an ordered sequence of tokens drawn from a defined vocabulary. As we saw in Lab 1, this is a fairly natural representation for **music**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsHFMNlRK6lm"
      },
      "source": [
        "## Sequence generation\n",
        "\n",
        "There are many tasks you can perform with a trained language model, but one of the most straightforward and natural is that of generating more text in that language. Starting with some initial **primer** sequence to provide context, an arbitrarily long sequence can be generated by predicting the next word, and then feeding the new augmented sentence including the new word back as context for the next word, in turn. At each point the next word can be predicted either by just choosing the single most likely candidate, or randomly choosing among multiple candidates, with the probability of each choice set according to what the model thinks is the probability of that word occurring next.\n",
        "\n",
        "This kind of sequence generation has been a staple of social media humour in recent years, usually framed along the lines of:\n",
        "\n",
        "> We trained an AI on horror movie titles/Taylor Swift lyrics/Donald Trump's tweets and look what it came up with ðŸ˜‚ðŸ¤£ðŸ˜‚ðŸ¤ª!!!\n",
        "\n",
        "Such sequence generation gags often make use of a kind of recurrent neural network known as **long short-term memory** or [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory), whose design dates back to the mid 1990s. For our musical experiments here, we're instead going to use a simple version of the more recent [Transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) architecture â€” which also underpins GPT-3.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFxcn4kc3fFX"
      },
      "source": [
        "# Setting up\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPdeOq8T9oU7"
      },
      "source": [
        "## Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swy8stqwXdma"
      },
      "source": [
        "Get the default (Bach, Mozart & Handel) corpus generated in Lab 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJwVFHjMXPSB"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/comp0161/labs_data.git data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCZ9K1cTXDug"
      },
      "source": [
        "If you played with the settings or code to produce a different corpus, you can upload that to train with instead. Use the file browser (the folder icon in the sidebar) to do so. Either overwrite the `data/corpus.txt` file or upload your file with a different name and change the `CORPUS` variable in the configuration section below to match.\n",
        "\n",
        "If you modified the encoding method used to produce your corpus, then you should also supply updated `text_to_music` and `music_to_text` functions to handle conversion between the text tokens and a Music21 Stream."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OTAJ-4u5pKE"
      },
      "source": [
        "## Music handling\n",
        "\n",
        "We'll use the same packages for music handling as last week: [Music21](https://web.mit.edu/music21/) for music streams and MIDI, [MuseScore](https://musescore.org/en) for notation and [FluidSynth](https://www.fluidsynth.org) for audio rendering.\n",
        "\n",
        "(As before, we discard a lot of installation messages here; if there are problems, remove the `> /dev/null` or `&> /dev/null`to help diagnose what's going wrong.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mfTK0gOgHCB"
      },
      "outputs": [],
      "source": [
        "# software for rendering music notation\n",
        "print('installing musescore')\n",
        "!yes | add-apt-repository ppa:mscore-ubuntu/mscore3-stable > /dev/null\n",
        "!apt update &> /dev/null\n",
        "!apt install musescore3 &> /dev/null\n",
        "print(\"done\")\n",
        "\n",
        "# software for rendering MIDI to WAV\n",
        "print('installing fluidsynth...')\n",
        "!apt-get install fluidsynth > /dev/null\n",
        "!cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2\n",
        "print('done')\n",
        "\n",
        "# install the music21 package for reading and transforming the MIDI training data\n",
        "# (an older version seems to be already installed on Colab as of this writing, but\n",
        "# we want to be up to date)\n",
        "print('upgrading music21')\n",
        "%pip install --upgrade music21 > /dev/null\n",
        "print('done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNBHYIYk90Xs"
      },
      "source": [
        "## Deep learning\n",
        "\n",
        "To learn patterns in the data we're going to use a Transformer model - a very slimmed-down version of huge (and hype-laden) language models such as [GPT-2](https://en.wikipedia.org/wiki/GPT-2) and [GPT-3](https://en.wikipedia.org/wiki/GPT-3). There are various implementations of these kinds of models around, but we'll make use of Andrej Karpathy's [minimalist implementation of the GPT family](https://github.com/karpathy/minGPT), which is neat and clear and pretty easy to use. It isn't (yet?) available on PyPI, so we'll install it from GitHub.\n",
        "\n",
        "<details>\n",
        "<summary>Note</summary>\n",
        "<p>There is some risk of breaking changes being introduced to the repo between the time of writing (late Nov 2022) and the actual lab session (expected late Feb 2023) so for safety we checkout the current repo head. Might need to revisit this if useful fixes get committed in the meantime.</p>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wculP7rr2boS"
      },
      "outputs": [],
      "source": [
        "# install Andrej Karpathy's minimal GPT implementation\n",
        "!git clone https://github.com/karpathy/minGPT.git\n",
        "%cd minGPT\n",
        "# checkout a known version for safety\n",
        "!git checkout 7218bcf --quiet\n",
        "%pip install -e .\n",
        "\n",
        "import sys, os, os.path\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3t1meXs16bd"
      },
      "source": [
        "## Python library imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v26FA5ePGrum"
      },
      "outputs": [],
      "source": [
        "# always useful imports\n",
        "import sys, os, os.path\n",
        "import copy\n",
        "import numpy as np\n",
        "import numpy.random\n",
        "import json\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# specialities\n",
        "import music21 as MU\n",
        "from IPython.display import Image, Audio\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "# NB: expects the MinGPT package to have been setup (see previous cell)\n",
        "from mingpt.model import GPT\n",
        "from mingpt.trainer import Trainer\n",
        "from mingpt.utils import set_seed, setup_logging, CfgNode\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUhvyN5w1_m8"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWnm8FRh2HNa"
      },
      "outputs": [],
      "source": [
        "# configuration variables\n",
        "SEED = 9907\n",
        "shared_rng = numpy.random.default_rng(seed=SEED)\n",
        "\n",
        "# dataset configuration\n",
        "COMPILED_DATA = 'data'\n",
        "CORPUS = os.path.join(COMPILED_DATA, 'corpus.txt')\n",
        "\n",
        "# corpus generation  defaults\n",
        "# we're not actually using most of these today since\n",
        "# the data is already generated, but we keep them here\n",
        "# because they appear in the code\n",
        "MIDI_DATA = 'classical_music_midi'\n",
        "COMPOSERS = 'bach,mozart,haydn'\n",
        "SIMPLIFY_LIMIT = 3\n",
        "SIMPLIFY_MODE = 'low'\n",
        "STRIP_TIES = True\n",
        "TOKENS_PER_LINE = 8\n",
        "\n",
        "\n",
        "# training configuration\n",
        "BLOCK_SIZE = 32\n",
        "MODEL_TYPE = 'gpt-nano'\n",
        "WORK_DIR = './out/nano_music'\n",
        "RESUME = True\n",
        "PROGRESS_INTERVAL = 10\n",
        "SAVE_INTERVAL = 200\n",
        "LEARNING_RATE = 5e-4\n",
        "\n",
        "# generator configuration\n",
        "GENERATE_COUNT = 300\n",
        "TEMPERATURE = 1.0\n",
        "SAMPLE = True\n",
        "TOP_K = 10\n",
        "EXPLICIT_PRIMER = None\n",
        "DEFAULT_PRIME_COUNT = 4\n",
        "\n",
        "# filenames for intermediate data\n",
        "MUSIC_MID = 'music.mid'\n",
        "MUSIC_WAV = 'music.wav'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTbpyQwUBsQI"
      },
      "source": [
        "## Display helpers\n",
        "\n",
        "We'll again use MusiC21's display features, but also provide a helper to render with FluidSynth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZB7hIWc7gJ_"
      },
      "outputs": [],
      "source": [
        "# helper function for rendering music in the notebook\n",
        "\n",
        "def fluid_play(music, rate=22050, midi_name=MUSIC_MID, wav_name=MUSIC_WAV):\n",
        "  \"\"\"\n",
        "  Write music to MIDI, then render that to WAV and display inline as Audio.\n",
        "\n",
        "  Note: if `None` is passed to `midi_name`, Music21 will invent a temp name.\n",
        "  \"\"\"\n",
        "  filename = music.write('mid')\n",
        "  os.rename(filename, midi_name)\n",
        "  !fluidsynth -ni font.sf2 $midi_name -F $wav_name -r $rate > /dev/null\n",
        "  display(Audio(wav_name))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuUD5_NJ2z46"
      },
      "source": [
        "## Data processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "147KTTkYIzNz"
      },
      "source": [
        "We'll re-use the processing functions from last week's lab. Not all of these are needed this time, because the data corpus is already prepared, but we'll just retain the whole lot for simplicity.\n",
        "\n",
        "**Important:** if you modified or created your own functions last week and are using a dataset built with those functions, substitute your versions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQ7aWaQwIfIH"
      },
      "outputs": [],
      "source": [
        "# functions for music processing\n",
        "\n",
        "def strip_ties ( s, inPlace=True ):\n",
        "    \"\"\"\n",
        "    Strip tied chords and drop non-starting tied notes from within chords.\n",
        "    NB: operates in place by default.\n",
        "\n",
        "    Intended for chordified streams, will probably produce weird\n",
        "    results otherwise.\n",
        "    \"\"\"\n",
        "    if not inPlace:\n",
        "        s = copy.deepcopy(s)\n",
        "\n",
        "    s.stripTies(inPlace=True)\n",
        "\n",
        "    for element in s.flatten():\n",
        "        if isinstance(element, MU.chord.Chord):\n",
        "            deletions = []\n",
        "            for note in element:\n",
        "                if note.tie is not None:\n",
        "                    if note.tie.type == 'start': note.tie = None\n",
        "                    else: deletions.append(note.pitch.nameWithOctave)\n",
        "\n",
        "            for note in deletions:\n",
        "                element.remove(note)\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "def simplify ( s, limit=SIMPLIFY_LIMIT, mode=SIMPLIFY_MODE, rng=shared_rng, inPlace=True ):\n",
        "    \"\"\"\n",
        "    Drop notes from big chords so they have no more than `limit` notes.\n",
        "    NB: operates in place by default.\n",
        "\n",
        "    Intended for chordified streams, will probably produce weird\n",
        "    results otherwise.\n",
        "\n",
        "    Drop strategies are pretty dumb. We always keep the highest and lowest notes\n",
        "    (crudely assumed to be melody and bass respectively). Notes are dropped from\n",
        "    the remainder according to one of three strategies:\n",
        "\n",
        "        'low': notes are dropped from low to high (the default)\n",
        "        'high': notes are dropped from high to low\n",
        "        'random': notes are dropped randomly\n",
        "\n",
        "    Latter could actually increase vocab by mapping the same input chord\n",
        "    to several outputs. Modes can be abbreviated to initial letters.\n",
        "    \"\"\"\n",
        "    if limit < 2: limit = 2\n",
        "\n",
        "    if not inPlace:\n",
        "        s = copy.deepcopy(s)\n",
        "\n",
        "    drop_func = {\n",
        "                    'r' : lambda d, c: rng.choice(d, c, replace=False),\n",
        "                    'h' : lambda d, c: d[(len(d)-c):]\n",
        "                }.get(mode.lower()[0],\n",
        "                      lambda d, c: d[:(c-len(d))])\n",
        "\n",
        "    for element in s.flatten():\n",
        "        if isinstance(element, MU.chord.Chord):\n",
        "            if len(element) > limit:\n",
        "                drop_count = len(element) - limit\n",
        "                drops = [ nn.pitch.nameWithOctave for nn in element ][1:-1]\n",
        "\n",
        "                if len(drops) > drop_count:\n",
        "                    drops = drop_func(drops, drop_count)\n",
        "\n",
        "                for note in drops:\n",
        "                    element.remove(note)\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "def music_to_text ( s ):\n",
        "    \"\"\"\n",
        "    Convert music stream into a list of text tokens defining the\n",
        "    chords, notes and rests and their durations.\n",
        "\n",
        "    Intended for chordified streams, will probably produce weird\n",
        "    results otherwise.\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for element in s.flatten():\n",
        "        name = None\n",
        "        if isinstance(element, MU.chord.Chord):\n",
        "            name = '.'.join(n.nameWithOctave for n in element.pitches)\n",
        "        elif isinstance(element, MU.note.Rest):\n",
        "            name = 'rest'\n",
        "        elif isinstance(element, MU.note.Note):\n",
        "            name = str(element.nameWithOctave)\n",
        "\n",
        "        if name is not None:\n",
        "            # convert any stray empty notes or chords into rests\n",
        "            name = name or 'rest'\n",
        "            result.append(f'{name};{float(element.duration.quarterLength):.6g}')\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def text_to_music( t ):\n",
        "    \"\"\"\n",
        "    Convert a sequence of text tokens into a music stream.\n",
        "    \"\"\"\n",
        "    result = MU.stream.Stream()\n",
        "\n",
        "    for element in t:\n",
        "        notes, quarters = element.split(';')\n",
        "        duration = MU.duration.Duration(float(quarters))\n",
        "\n",
        "        if '.' in notes:\n",
        "            notes = notes.split('.')\n",
        "            chord = []\n",
        "            for nn in notes:\n",
        "                note = MU.note.Note(nn)\n",
        "                note.duration = duration\n",
        "                chord.append(note)\n",
        "            result.append(MU.chord.Chord(chord))\n",
        "        elif notes == 'rest':\n",
        "            note = MU.note.Rest()\n",
        "            note.duration = duration\n",
        "            result.append(note)\n",
        "        else:\n",
        "            note = MU.note.Note(notes)\n",
        "            note.duration = duration\n",
        "            result.append(note)\n",
        "\n",
        "    return result\n",
        "\n",
        "def tokenise ( file, simplify_limit=SIMPLIFY_LIMIT, simplify_mode=SIMPLIFY_MODE,\n",
        "               do_strip=STRIP_TIES, rng=shared_rng ):\n",
        "    \"\"\"\n",
        "    Read a MIDI file and convert to text tokens, with\n",
        "    optional preprocessing.\n",
        "    \"\"\"\n",
        "    raw_stream = MU.converter.parse(file)\n",
        "    chorded = raw_stream.chordify()\n",
        "\n",
        "    if do_strip:\n",
        "        strip_ties(chorded)\n",
        "\n",
        "    if simplify_limit:\n",
        "        simplify(chorded, simplify_limit, simplify_mode, rng=rng)\n",
        "\n",
        "    return music_to_text(chorded)\n",
        "\n",
        "\n",
        "def build_dataset ( midi_path=MIDI_DATA, composers=COMPOSERS, out_path=None,\n",
        "                    do_strip=STRIP_TIES, simplify_limit=SIMPLIFY_LIMIT,\n",
        "                    simplify_mode=SIMPLIFY_MODE, rng=shared_rng ):\n",
        "    \"\"\"\n",
        "    Construct a tokenised data file of optionally simplified and tie-stripped\n",
        "    music by the specified composers, for use in training a language model.\n",
        "    \"\"\"\n",
        "    if composers:\n",
        "        comps = composers.split(',')\n",
        "    else:\n",
        "        comps = [ ff for ff in os.listdir(midi_path) if os.path.isdir(os.path.join(midi_path, ff)) ]\n",
        "\n",
        "    if out_path is None:\n",
        "        out_path = os.path.join(COMPILED_DATA, '_'.join(comps) + '.txt')\n",
        "\n",
        "    out_dir = os.path.split(out_path)[0]\n",
        "    if not os.path.isdir(out_dir):\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    tokens = []\n",
        "    count = 0\n",
        "\n",
        "    for comp in comps:\n",
        "        dir = os.path.join(midi_path, comp)\n",
        "        for filename in os.listdir(dir):\n",
        "            if filename.lower().endswith('.mid'):\n",
        "                print(f'reading {filename}')\n",
        "                tokens.extend(tokenise(os.path.join(dir, filename),\n",
        "                                       simplify_limit=simplify_limit,\n",
        "                                       simplify_mode=simplify_mode,\n",
        "                                       do_strip=do_strip,\n",
        "                                       rng=rng))\n",
        "                count += 1\n",
        "\n",
        "    print(f'loaded {len(tokens)} tokens from {count} files')\n",
        "\n",
        "    print(f'writing tokens to {out_path}')\n",
        "\n",
        "    with open(out_path, 'w') as f:\n",
        "        for off in range(0, len(tokens), TOKENS_PER_LINE):\n",
        "            print(' '.join(tokens[off:off+TOKENS_PER_LINE]), file=f)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbyuIwYFHftU"
      },
      "source": [
        "# Learning\n",
        "\n",
        "In this section we'll first define the classes and functions that we'll use to do the actual model training and music generation. Then we'll finally run them at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo7oPQkrI1AA"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "MinGPT uses a fairly simple tree structure with attributes on nodes to store configuration details. It provides the useful feature of merging configuration from command line args, which is a bit less relevant here. Nevertheless, we'll stick with it, allowing an optional list of configuration arguments to be supplied to `configure`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euL-flzt5HqT"
      },
      "outputs": [],
      "source": [
        "def get_default_config():\n",
        "    \"\"\"\n",
        "    Default configuration for both training and generation\n",
        "    is basically filled out from the config vars defined earlier.\n",
        "    \"\"\"\n",
        "    config = CfgNode()\n",
        "    config.system = CfgNode()\n",
        "    config.system.seed = SEED\n",
        "    config.system.work_dir = WORK_DIR\n",
        "    config.system.resume = RESUME\n",
        "    config.system.reloading = False\n",
        "    config.system.model_path = os.path.join(config.system.work_dir, 'model.pt')\n",
        "    config.data = LangDataset.get_default_config()\n",
        "\n",
        "    config.model = GPT.get_default_config()\n",
        "    config.model.model_type = MODEL_TYPE\n",
        "\n",
        "    config.trainer = Trainer.get_default_config()\n",
        "    config.trainer.learning_rate = LEARNING_RATE\n",
        "    config.trainer.progress_interval = PROGRESS_INTERVAL\n",
        "    config.trainer.save_interval = SAVE_INTERVAL\n",
        "\n",
        "    # use fewer workers to suppress an issue with Colab in 2024\n",
        "    config.trainer.num_workers = 2\n",
        "\n",
        "    config.generator = CfgNode()\n",
        "    config.generator.count = GENERATE_COUNT\n",
        "    config.generator.temperature = TEMPERATURE\n",
        "    config.generator.sample = SAMPLE\n",
        "    config.generator.top_k = TOP_K\n",
        "    config.generator.explicit_primer = EXPLICIT_PRIMER\n",
        "    config.generator.prime_offset = 0\n",
        "    config.generator.prime_count = DEFAULT_PRIME_COUNT\n",
        "\n",
        "    return config\n",
        "\n",
        "\n",
        "def configure (args=[]):\n",
        "    \"\"\"\n",
        "    Get configuration for training or generating from a model.\n",
        "\n",
        "    The starting configuration will be that returned by `get_default_config`\n",
        "    but if the specified model already exists this will be substituted with\n",
        "    the config saved for that model, *unless* system.resume is set to False.\n",
        "\n",
        "    Either way, the resulting config can then be modified by `args`, a list\n",
        "    of strings of the form:\n",
        "\n",
        "      --path.to.option=value\n",
        "\n",
        "    This will explicitly set the specified option to the specified value,\n",
        "    overwriting the existing one. If the option does not exist, an exception\n",
        "    will be raised.\n",
        "    \"\"\"\n",
        "    config = get_default_config()\n",
        "\n",
        "    # we merge from args into the default before checking for reloading\n",
        "    # in case the args include --system.resume=False\n",
        "    config.merge_from_args(args)\n",
        "\n",
        "    # as long as resume hasn't been turned off, load the model if it exists\n",
        "    reloading = config.system.resume and os.path.exists(config.system.model_path)\n",
        "\n",
        "    if reloading:\n",
        "        with open(os.path.join(config.system.work_dir, 'config.json'), 'r') as f:\n",
        "            loaded = json.load(f)\n",
        "\n",
        "        # annoyingly, CfgNode doesn't support updating from a nested dict\n",
        "        # in the same way it does for args, so we need to update the\n",
        "        # subsidiary nodes explicitly\n",
        "        if 'system' in loaded: config.system.merge_from_dict(loaded['system'])\n",
        "        if 'generator' in loaded: config.generator.merge_from_dict(loaded['generator'])\n",
        "        if 'data' in loaded: config.data.merge_from_dict(loaded['data'])\n",
        "        if 'model' in loaded: config.model.merge_from_dict(loaded['model'])\n",
        "        if 'trainer' in loaded: config.trainer.merge_from_dict(loaded['trainer'])\n",
        "\n",
        "        # note intent to reload parameters\n",
        "        config.system.reloading = True\n",
        "\n",
        "        # we still want args to have priority, so merge *again*\n",
        "        config.merge_from_args(args)\n",
        "\n",
        "    return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaPEmszmKZIF"
      },
      "source": [
        "## LangDataset\n",
        "\n",
        "A MinGPT Dataset subclass that manages loading the training data and converting to and from PyTorch tensors of integer indices. This is not music specific, it will load any text file of whitespace-delimited tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lM4Vx86LCQu"
      },
      "outputs": [],
      "source": [
        "class LangDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Loads a single whitespace-delimited text file of tokens,\n",
        "    manages translation to and from integer indices, and delivers\n",
        "    blocks in encoded (integer index) form.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        config = CfgNode()\n",
        "        config.block_size = BLOCK_SIZE\n",
        "        config.source = CORPUS\n",
        "        return config\n",
        "\n",
        "    def __init__(self, config, load=True):\n",
        "        self.config = config\n",
        "        if load:\n",
        "            self.load()\n",
        "        else:\n",
        "            self.data = []\n",
        "            self.vocab = []\n",
        "            self.encoding = {}\n",
        "            self.decoding = {}\n",
        "            self.encoded = []\n",
        "\n",
        "    def load(self):\n",
        "        print(f'loading from {self.config.source}')\n",
        "        with open(self.config.source, 'r') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        self.data = text.split()\n",
        "        self.vocab = sorted(list(set(self.data)))\n",
        "        self.encoding = { w:i for i,w in enumerate(self.vocab) }\n",
        "        self.decoding = { i:w for i,w in enumerate(self.vocab) }\n",
        "        self.encoded = self.encode(self.data)\n",
        "\n",
        "        print(f'data has {len(self.encoded)} words, with vocabulary of {len(self.vocab)}')\n",
        "\n",
        "    def encode(self, seq):\n",
        "        \"\"\"\n",
        "        Map a sequence of word tokens to numerical indices.\n",
        "        \"\"\"\n",
        "        return [ self.encoding[w] for w in seq ]\n",
        "\n",
        "    def decode(self, seq):\n",
        "        \"\"\"\n",
        "        Map a sequence of numerical indices to work tokens.\n",
        "        \"\"\"\n",
        "        return [ self.decoding[i] for i in seq ]\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.config.block_size\n",
        "\n",
        "    def save_vocab(self, file):\n",
        "        \"\"\"\n",
        "        Save vocabulary (in order) as a simple newline delimited text file.\n",
        "        \"\"\"\n",
        "        with open(file, 'w') as f:\n",
        "            print('\\n'.join(self.vocab), file=f)\n",
        "\n",
        "    def load_vocab(self, file):\n",
        "        \"\"\"\n",
        "        Load vocabulary from a whitespace delimited text file.\n",
        "        (This will load a file created by `save_vocab` but is slightly\n",
        "        more general.)\n",
        "        \"\"\"\n",
        "        with open(file, 'r') as f:\n",
        "            text = f.read()\n",
        "        self.vocab = text.split()\n",
        "        self.encoding = { w:i for i,w in enumerate(self.vocab) }\n",
        "        self.decoding = { i:w for i,w in enumerate(self.vocab) }\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        How big is the dataset in terms of loadable chunks.\n",
        "\n",
        "        NB: chunks require block size + 1 because label is sequence\n",
        "        shifted forward by one token.\n",
        "        \"\"\"\n",
        "        return len(self.data) - self.config.block_size - 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a data, label pair as tensors of integer indices.\n",
        "        \"\"\"\n",
        "        chunk = self.encoded[idx:idx + self.config.block_size + 1]\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNDThyTeBc96"
      },
      "source": [
        "## Learning functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRFFeO0p0esA"
      },
      "source": [
        "The functions below handle model configuration, training a model according to the configuration, and then loading a trained model and using it generate a music sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Dv6XT34Bj3n"
      },
      "outputs": [],
      "source": [
        "def setup_model(args=[]):\n",
        "    \"\"\"\n",
        "    Shared initialisation for training and generation.\n",
        "    Configures, loads data and builds model.\n",
        "    \"\"\"\n",
        "    config = configure(args)\n",
        "    setup_logging(config)\n",
        "    set_seed(config.system.seed)\n",
        "\n",
        "    print('loading data')\n",
        "    dataset = LangDataset(config.data)\n",
        "\n",
        "    print('building model')\n",
        "    config.model.vocab_size = dataset.get_vocab_size()\n",
        "    config.model.block_size = dataset.get_block_size()\n",
        "    model = GPT(config.model)\n",
        "\n",
        "    if config.system.reloading:\n",
        "        print(f'reloading trained parameters from {config.system.model_path}')\n",
        "        model.load_state_dict(torch.load(config.system.model_path))\n",
        "\n",
        "    return config, dataset, model\n",
        "\n",
        "\n",
        "def train_model(args=[]):\n",
        "    \"\"\"\n",
        "    Train (or resume training) a model.\n",
        "\n",
        "    By default, training will keep running until interrupted.\n",
        "    Set --trainer.max_iters=<number> to curtail this.\n",
        "    \"\"\"\n",
        "    config, dataset, model = setup_model(args)\n",
        "\n",
        "    print('initialising trainer')\n",
        "    trainer = Trainer(config.trainer, model, dataset)\n",
        "\n",
        "    def batch_end_callback(trainer):\n",
        "        if trainer.iter_num % config.trainer.progress_interval == 0:\n",
        "            print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
        "\n",
        "        if trainer.iter_num % config.trainer.save_interval == 0:\n",
        "            torch.save(model.state_dict(), config.system.model_path)\n",
        "\n",
        "    trainer.set_callback('on_batch_end', batch_end_callback)\n",
        "\n",
        "    print('running trainer')\n",
        "    trainer.run()\n",
        "\n",
        "def generate(args=[]):\n",
        "    \"\"\"\n",
        "    Generate and return a music sequence using an existing trained model.\n",
        "    \"\"\"\n",
        "    config, dataset, model = setup_model(args)\n",
        "\n",
        "    # no training here\n",
        "    model.eval()\n",
        "\n",
        "    if config.generator.explicit_primer:\n",
        "        # encode supplied explicit fragment\n",
        "        # NB: will fail if it includes words outside vocab\n",
        "        primer = dataset.encode(config.generator.explicit_primer.split())\n",
        "    else:\n",
        "        # use some fragment of the training data\n",
        "        start = config.generator.prime_offset\n",
        "        end = start + config.generator.prime_count\n",
        "        primer = dataset.encoded[start:end]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor(primer, dtype=torch.long)[None, ...]\n",
        "        y = model.generate(x, config.generator.count,\n",
        "                           temperature=config.generator.temperature,\n",
        "                           do_sample=config.generator.sample,\n",
        "                           top_k=config.generator.top_k)[0]\n",
        "\n",
        "    print(y)\n",
        "    output = dataset.decode(y.numpy())\n",
        "    print(' '.join(output))\n",
        "\n",
        "    return text_to_music(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFUWlafrBsN7"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "OK, let's give it try."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apwg07ppBwel"
      },
      "outputs": [],
      "source": [
        "train_model(['--trainer.max_iters=20001', '--trainer.progress_interval=100', '--system.resume=False'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Uq2WKvVRP7j"
      },
      "source": [
        "Check the expected outputs have been created. The directory should contain at least 3 files, the most important one being `model.pt`, which contains the actual trained model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Riz25pbjHgza"
      },
      "outputs": [],
      "source": [
        "!ls -l $WORK_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SAcwkB8CFru"
      },
      "source": [
        "## Generation\n",
        "\n",
        "Having trained a model, let's generate some music from it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQm-Et_MCH8C"
      },
      "outputs": [],
      "source": [
        "# using the defaults: prime with the first 4 notes of the corpus\n",
        "generated = generate([])\n",
        "generated.show()\n",
        "generated.show('midi')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "355UqdCHVzZU"
      },
      "source": [
        "We can also specify an explicit primer in text notation (with the proviso that all the chord+duration elements must actually occur in the training corpus). Here's a simple fragment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mk542KwSoPw"
      },
      "outputs": [],
      "source": [
        "primer = text_to_music('C3.E3.G4;0.25 F3;0.25 G3;0.25 A3;0.25'.split())\n",
        "primer.show()\n",
        "primer.show('midi')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDjRbPUDWRUM"
      },
      "source": [
        "And we generate an ongoing sequence from it thus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22qA0CvTTXzn"
      },
      "outputs": [],
      "source": [
        "generated = generate(['--generator.explicit_primer=\"C3.E3.G4;0.25 F3;0.25 G3;0.25 A3;0.25\"'])\n",
        "generated.show()\n",
        "generated.show('midi')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzEZQAw4ccWh"
      },
      "source": [
        "## Render and download your results\n",
        "\n",
        "In the above examples we don't explicitly write the generated music to files. But we can do using the `fluid_play` function defined earlier. When you call this function on a music stream, by default the MIDI data is written to a file called `music.mid` and the rendered audio is written to `music.wav`. You can then download both files for future use. (You'll need `music.mid` for next week's lab.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_pqibxhcjrk"
      },
      "outputs": [],
      "source": [
        "fluid_play(generated)\n",
        "files.download('music.mid')\n",
        "files.download('music.wav')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrrH9Xp6Gq65"
      },
      "source": [
        "# Playing around"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLqT5iWaHqtg"
      },
      "source": [
        "## Change the generation settings\n",
        "\n",
        "The sequence generation process has a number of settings that can be tweaked to affect what gets produced. The default configuration is defined in the code above, but the settings can be overridden by the arguments passed to `generate()`, as shown in the cell below.\n",
        "\n",
        "* `generator.count`: the number of tokens to generate, ie the length of the resulting sequence.\n",
        "* `generator.sample`: whether to sample randomly from possible next tokens. If this is `False`, then the most likely next token is always chosen.\n",
        "* `generator.temperature`: how strongly the token choice is affected by the likelihood. Higher temperatures increase the chance of picking less likely tokens, making the output sequence more \"random\". Only relevant when `generator.sample` is `True`.\n",
        "* `generator.top_k`: restrict the sampling to this many of the most likely candidates. The higher this number, the more tokens are considered (though their probability of being chosed may be low). Only relevant when `generator.sample` is `True`.\n",
        "* `generator.explicit_primer`: specify a starting sequence of tokens explicitly as a text string. See the example above. Tokens in the supplied primer must be present in the corpus.\n",
        "* `generator.prime_offset`: Instead of an explicit primer, start from a sequence of tokens taken directly from the corpus, starting at this offset. Only relevant is `generator.explicit_primer` is `None`.\n",
        "* `generator.prime_count`: Number of tokens from the corpus to use as primer. Only relevant if `generator.explicit_primer` is `None`.\n",
        "* `system.seed`: Seed value for the random number generator. Varying this should give rise to different sequences if sampling is used â€” though they might not be *very* different if the learned model contains highly stereotyped sequences.\n",
        "\n",
        "Note that any setting you don't explicitly override will default to its most recently specified value. So for example if you've used an explicit primer (as in the cell above) and don't want to use it again, you should explictly pass `'--generator.explicit_primer=None'` rather than omitting the setting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xb-qQJb9D9BY"
      },
      "outputs": [],
      "source": [
        "generated = generate([\n",
        "    '--generator.count=300',\n",
        "    '--generator.temperature=1.0',\n",
        "    '--generator.sample=True',\n",
        "    '--generator.top_k=10',\n",
        "    '--generator.explicit_primer=None',\n",
        "    '--generator.prime_offset=0',\n",
        "    '--generator.prime_count=4',\n",
        "    '--system.seed=9907'\n",
        "    ])\n",
        "generated.show('midi')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqgi52yxH1a9"
      },
      "source": [
        "## Change the model & training settings\n",
        "\n",
        "As with generation, the training process is controlled by a number of settings, which can be modified by passing arguments to the `train_model` function:\n",
        "\n",
        "* `model.model_type`: which kind of model to train. All model types use the same underlying structures but vary in size and complexity. There are several types defined, some of them unfathomably huge. Larger models can learn more complex behaviour but are much slower to train and require much more training data to be useful. For our purposes only the following are worth considering, and even `mini` is almost certainly vastly over-specified:\n",
        "    * `gpt-nano`: 3 layers, 3 heads, 48 embedding dimensions (this is the default)\n",
        "    * `gpt-micro`: 4 layers, 4 heads, 128 embedding dimensions\n",
        "    * `gpt-mini`: 6 layers, 6 heads, 192 embedding dimensions\n",
        "\n",
        "  (Don't worry too much about what \"layers\", \"heads\" and \"embedding dimensions\" are, just think of them as indicators of **bigness**.)\n",
        "* `data.source`: a corpus file to use as training data. This should be a whitespace-delimited text file in which the tokens are whatever is understood by `text_to_music` and `music_to_text`.\n",
        "* `trainer.max_iters`: how many training iterations to run. Training for more iterations will take longer but will lead to improved learning (at least, up to a point).\n",
        "* `trainer.learning_rate`: how much the model parameters are updated each iteration. Larger values might learn faster but may also fail to converge. You should probably keep this fairly small, say < 0.01.\n",
        "* `trainer.weight_decay`: how much L2 regularisation to apply to the model parameters. Regularisation combats **overfitting** (a tendency to just memorise the training set, leading to poor generalisation) but can lead to **underfitting** (a failure to adequately capture the behaviour of the data).\n",
        "* `trainer.progress_interval`: how often to print an update message when training. You probably don't care about this.\n",
        "* `trainer.save_interval`: how often to save the model during training. This can be useful when training is slow, but you probably don't care much about this either.\n",
        "* `system.seed`: Seed value for the random number generator. Again, you probably don't care about this.\n",
        "* `system.resume`: whether to initialise the model with the previous trained state.\n",
        "    \n",
        "    **IMPORTANT:** when training a new model with a new configuration (ie, pretty much always), you should explicitly set this to `False`, as shown in the code cell below\n",
        "\n",
        "Once you've trained a model, you can then go back to generating from it to explore what has been learned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR5H8vAtT9y7"
      },
      "outputs": [],
      "source": [
        "train_model([\n",
        "    '--model.model_type=\"gpt-nano\"',\n",
        "    '--data.source=\"data/corpus.txt\"',\n",
        "    '--trainer.learning_rate=5e-4',\n",
        "    '--trainer.weight_decay=0.1',\n",
        "    '--trainer.max_iters=10001',\n",
        "    '--trainer.progress_interval=100',\n",
        "    '--trainer.save_interval=200',\n",
        "    '--system.seed=9907',\n",
        "    '--system.resume=False'\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVkjJLJRRN63"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "While you're waiting for your model to train, you might want to think & talk about:\n",
        "* What sort of results do you expect from this process?\n",
        "\n",
        "Once you've actually been able to generate something:\n",
        "* Were your expectations correct?\n",
        "* Are the results in any sense \"musical\"?\n",
        "* Are there fragments, or even whole sections, of tunes you recognise?\n",
        "* Does the style seem consistent?\n",
        "* Is there evidence of long range structure in the generated music?\n",
        "* Are any features or qualities obviously missing?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt8pblHJRbPu"
      },
      "source": [
        "# Further work\n",
        "\n",
        "In all likelihood, tweaking the model settings and perhaps trying a different data corpus will be more than enough to keep you busy. But if you're feeling excessively motivated, you *could* try replacing the GPT implementation used here with something else entirely â€” perhaps the LSTM model mentioned above. Doing so will require quite a bit of coding, but the PyTorch framework used here does provide a useful [LSTM layer](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) implementation â€” a good starting point if you want to get your hands dirty."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTvqCFEEryieirYIoadOMb",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}